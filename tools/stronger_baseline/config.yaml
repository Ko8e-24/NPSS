DATA_ROOT: '../datasets/'
LOGS_ROOT: '../logs/'


MODEL:
  # architecture
  backbone: 'resnet50'
  pooling: 'gem'
  embed_feat: 0
  dropout: 0.

  # (Optional) for stronger baseline
  num_parts: 0
  include_global: True  # support True only
  # (Optional) for local embedding layers
  loc_embed_feat: 0

  dsbn: True

  sync_bn: True
  samples_per_bn: 16

  mean_net: False
  alpha: 0.999

  # pretraining
  imagenet_pretrained: True
  source_pretrained: null
  # source_pretrained: ../logs/LUPerson/lup_moco_r50.pth
  # source_pretrained: ../logs/source_pretrain/msmt17/model_best.pth
  # source_pretrained: ../logs/source_pretrain_wo_autoaug_w_erase/msmt17/model_best.pth
  # source_pretrained: ../logs/stronger_baseline/pretrain/msmt17/model_best.pth
  # source_pretrained: ../logs/stronger_baseline/pretrain_g/market1501/model_best.pth
  # source_pretrained: ../logs/stronger_baseline/pretrain/market1501/model_best.pth


DATA:

  height: 256
  width: 128
  norm_mean: [0.485, 0.456, 0.406]
  norm_std: [0.229, 0.224, 0.225]

  TRAIN:
    # augmentation
    is_autoaug: False

    is_flip: True
    flip_prob: 0.5

    is_pad: True
    pad_size: 10

    is_blur: False
    blur_prob: 0.5

    is_erase: True
    erase_prob: 0.5

    # dual augmentation for MMT
    is_mutual_transform: False
    mutual_times: 2

    # cross-domain mixup
    is_mixup: False
    mixup_alpha: 0.8


TRAIN:
  seed: 1
  deterministic: True
  # mixed precision training for PyTorch>=1.6
  amp: False

  # datasets
  # datasets: {'msmt17': 'trainval',}
  # datasets: {'market1501': 'trainval',}
  # datasets: {'dukemtmcreid': 'trainval', }
  # datasets: {'market1501': 'trainval', 'dukemtmcreid': 'trainval'}
  # datasets: {'msmt17': 'trainval', 'market1501': 'trainval'}
  # datasets: {'market1501': 'trainval', 'msmt17': 'trainval'}
  # datasets: {'msmt17': 'trainval', 'dukemtmcreid': 'trainval', 'market1501': 'trainval'}
  datasets: {'msmt17': 'trainval', 'dukemtmcreid': 'trainval',}
  # datasets: {'dukemtmcreid': 'trainval', 'market1501': 'trainval'}
  # datasets: {'dukemtmcreid': 'trainval', 'msmt17': 'trainval'}
  unsup_dataset_indexes: [1,]
  # unsup_dataset_indexes: [0,]
  # unsup_dataset_indexes: [2,]
  # unsup_dataset_indexes: []

  epochs: 50
  iters: 400

  LOSS:
    lmd_local: 1.0
    # weights of losses of local branches
    include_local_triplet_loss: False
    # whether include local triplet loss

    # losses: {'cross_entropy': 1., 'softmax_triplet': 1., 'instance_memory': 0.02}
    # include instance_memory_loss
    # losses: {'cross_entropy': 1.}
    # losses: {'cross_entropy': 1., 'softmax_triplet': 1.}
    # losses: {'cross_entropy': 1., 'soft_entropy': 1., 'softmax_triplet': 1.}
    losses: {'cross_entropy': 1., 'soft_entropy': 1., 'softmax_triplet': 1., 'inter_instance_soft_entropy': 1.}
    # losses: {'cross_entropy': 1., 'soft_entropy': 0.1, 'softmax_triplet': 1.0, 'soft_softmax_triplet': 0.1}
    margin: 0.



  # validate
  # val_dataset: 'market1501'
  val_dataset: 'dukemtmcreid'
  # val_dataset: 'msmt17'
  val_freq: 5

  # sampler
  SAMPLER:
    num_instances: 4
    is_shuffle: True

  # data loader
  LOADER:
    samples_per_gpu: 16
    workers_per_gpu: 2

  # pseudo labels
  PSEUDO_LABELS:
    freq: 1 # epochs
    use_outliers: False
    norm_feat: True
    norm_center: True

    cluster: 'dbscan'
    # independent clustering for every global and local branches,
    # instead of only use global branch for clustering
    cluster_per_branch: False
    eps: [0.6, 0.7]
    min_samples: 4 # for dbscan
    dist_metric: 'jaccard'
    k1: 30 # for jaccard distance
    k2: 6 # for jaccard distance
    search_type: 0 # 0,1,2 for GPU, 3 for CPU (work for faiss)
    cluster_num: null


  # optim
  OPTIM:
    optim: 'adam'
    lr: 0.00035
    weight_decay: 0.0005

  SCHEDULER:
    # lr_scheduler: null  # original
    lr_scheduler: 'single_step'
    stepsize: 20
    gamma: 0.1


TEST:

  # datasets
  datasets: ['market1501', 'dukemtmcreid', 'msmt17']
  # datasets: ['market1501', 'dukemtmcreid']
  # datasets: ['market1501']

  # data loader
  LOADER:
    samples_per_gpu: 32
    workers_per_gpu: 2

  # ranking setting
  dist_metric: 'euclidean'
  norm_feat: True
  dist_cuda: True

  # post processing
  rerank: False
  search_type: 0 # 0,1,2 for GPU, 3 for CPU (work for faiss)
  k1: 20
  k2: 6
  lambda_value: 0.3
